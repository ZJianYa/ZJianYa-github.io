
# 事务

其实就是保证数据一致性的一种办法。  
三次握手、四次握手都是为了可靠性。  

ACID（Atomicity、Consistency、Isolation、Durability，即原子性、一致性、隔离性、持久性）

## 四个级别和事务特性

当数据库上有多个事务同时执行的时候，就可能出现脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题，为了解决这些问题，就有了“隔离级别”的概念。

SQL 标准的事务隔离级别包括：读未提交（read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（serializable ）。下面我逐一为你解释：

- 读未提交是指，一个事务还没提交时，它做的变更就能被别的事务看到。  
- 读提交是指，一个事务提交之后，它做的变更才会被其他事务看到。  
- 可重复读是指，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。
- 串行化，顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。

其中“读提交”和“可重复读”比较难理解，所以我用一个例子说明这几种隔离级别。假设数据表 T 中只有一列，其中一行的值为 1，下面是按照时间顺序执行两个事务的行为。  

```{}
mysql> create table T(c int) engine=InnoDB;
insert into T(c) values(1);
```

在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。  
在“读提交”隔离级别下，这个视图是在每个 SQL 语句开始执行的时候创建的（看到的外部是动态的）。  
在“可重复读”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图（看到的外部是静态的）。  
这里需要注意的是，“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念。  
而“串行化”隔离级别下直接用加锁的方式来避免并行访问（外部是被暂停的，和“看到外部是静态”不同的是：那个静态是看到的而外界并非真的是静态的，这个外部暂停确实真的暂停）。  

Oracle 数据库的默认隔离级别其实就是“读提交”  
MySQL 默认是“可重复度”  

mysql> show variables like 'transaction_isolation';

### 适用场景

“可重复度”  
假设你在管理一个个人银行账户表。一个表存了每个月月底的余额，一个表存了账单明细。这时候你要做数据校对，也就是判断上个月的余额和当前余额的差额，是否与本月的账单明细一致。你一定希望在校对过程中，即使有用户发生了一笔新的交易，也不影响你的校对结果。

### 事务隔离的实现

在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。  
这个是undo日志，也就可以读到之前的状态，即可以实现"可重复度"。

同一条记录在系统中可以存在多个版本（是在事务期间，确切说是在日志里因为事务的原因，会对同一条数据记录多个版本），就是数据库的多版本并发控制（MVCC）。  

### 回滚日志和长事件

你一定会问，回滚日志总不能一直保留吧，什么时候删除呢？答案是，在不需要的时候才删除。也就是说，系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。  
什么时候才不需要了呢？就是当系统里没有比这个回滚日志更早的 `read-view` 的时候。  

基于上面的说明，我们来讨论一下为什么建议你尽量不要使用长事务。  

长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。  

在 MySQL 5.5 及以前的版本，回滚日志是跟数据字典一起放在 ibdata 文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小。我见过数据只有 20GB，而回滚段有 200GB 的库。最终只好为了清理回滚段，重建整个库。

除了对回滚段的影响，长事务还占用锁资源，也可能拖垮整个库，这个我们会在后面讲锁的时候展开。

### 事务启动方式

显式启动事务语句， begin 或 start transaction。配套的提交语句是 commit，回滚语句是 rollback。  

set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。

有些客户端连接框架会默认连接成功后先执行一个 set autocommit=0 的命令。这就导致接下来的查询都在事务中，如果是长连接，就导致了意外的长事务。

因此，我会建议你总是使用 set autocommit=1, 通过显式语句的方式来启动事务。

在 autocommit 为 1 的情况下，用 begin 显式启动的事务，如果执行 commit 则提交事务。如果执行 commit work and chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行 begin 语句的开销。同时带来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。

你可以在 information_schema 库的 innodb_trx 这个表中查询长事务，比如下面这个语句，用于查找持续时间超过 60s 的事务。  
select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))>60

## 问题监控和排查

- 检测 general_log  
- 检测 information_schema.Innodb_trx  
- innodb_undo_tablespaces设置为2  

## 行锁

### 两阶段锁

在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。

如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。  

两阶段锁导致了死锁的产生的可能性。  

### 死锁和死锁检测

死锁产生的场景：当发生资源循环依赖就可以发生

- innodb_lock_wait_timeout 可以设置超时时间，默认是50s
- innodb_deadlock_detect 可以开启死锁检测功能，默认值本身就是 on
  
如果在热点数据，比如都更新某一行数据时  
所有被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是 O(n) 的操作。如果并非请求是 100 万量级的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的 CPU 资源。因此，你就会看到 CPU 利用率很高，但是每秒却执行不了几个事务。

一种头痛医头的方法，就是如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉。  
另一个思路是控制并发度：应用程序端：我自己的观点是可以用fixedThreadPool来解决，数据库端：更改源码设置队列，亦或者把数据库的热点数据拆分开来。  

### 总结

调整语句的执行顺序并不一定能解决死锁，但是可以减少整体上枷锁的时间。  
降低并发量可以有效降低死锁发生的概率。  

## DDL 和事务
